{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ede544",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ade5bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# dataset\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Functions to create the NN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "# library to the visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8bf6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load the data in variables\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1367278f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[364].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0e90b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "print(y_train[364])\n",
    "plt.imshow(x_train[364]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f7f1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of the classes\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1f429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ade96",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7259c626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578e2c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "# use a secuential model\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "\n",
    "# activation function to the layer\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "# dropout to the regularization\n",
    "model_1.add(Dropout(0.5))\n",
    "\n",
    "# layer of the output with a softmax\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab9cf575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 59s 34ms/step - loss: 1.7139 - accuracy: 0.3741 - val_loss: 1.4042 - val_accuracy: 0.4913\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4448 - accuracy: 0.4773 - val_loss: 1.2945 - val_accuracy: 0.5342\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.3436 - accuracy: 0.5184 - val_loss: 1.2647 - val_accuracy: 0.5434\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.2724 - accuracy: 0.5461 - val_loss: 1.2870 - val_accuracy: 0.5509\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.2248 - accuracy: 0.5667 - val_loss: 1.1203 - val_accuracy: 0.6026\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1880 - accuracy: 0.5827 - val_loss: 1.1184 - val_accuracy: 0.6085\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1586 - accuracy: 0.5910 - val_loss: 1.0793 - val_accuracy: 0.6192\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1375 - accuracy: 0.6028 - val_loss: 1.0454 - val_accuracy: 0.6357\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1173 - accuracy: 0.6072 - val_loss: 1.0862 - val_accuracy: 0.6292\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.1011 - accuracy: 0.6167 - val_loss: 1.0407 - val_accuracy: 0.6419\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0857 - accuracy: 0.6218 - val_loss: 1.1075 - val_accuracy: 0.6081\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0794 - accuracy: 0.6293 - val_loss: 1.0198 - val_accuracy: 0.6498\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 16s 11ms/step - loss: 1.0698 - accuracy: 0.6321 - val_loss: 1.0925 - val_accuracy: 0.6295\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0632 - accuracy: 0.6376 - val_loss: 1.2097 - val_accuracy: 0.6046\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0661 - accuracy: 0.6368 - val_loss: 1.2391 - val_accuracy: 0.5813\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0641 - accuracy: 0.6378 - val_loss: 1.2352 - val_accuracy: 0.5804\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.0596 - accuracy: 0.6408 - val_loss: 1.0486 - val_accuracy: 0.6433\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 17s 11ms/step - loss: 1.0580 - accuracy: 0.6433 - val_loss: 1.0358 - val_accuracy: 0.6439\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 18s 11ms/step - loss: 1.0549 - accuracy: 0.6419 - val_loss: 1.0194 - val_accuracy: 0.6534\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 18s 12ms/step - loss: 1.0543 - accuracy: 0.6449 - val_loss: 1.0445 - val_accuracy: 0.6466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1841766d2d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=20,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57569a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "055f5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sequential model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Adding the first conv layer\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "    # Adding the activation function\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Adding the second conv layer\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Maxpooling to reduce the features\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Adding the dropout for the maxpooling\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Adding the thrid conv layer\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Adding the thrid conv layer\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "\n",
    "# Maxpooling to reduce the features\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "# Adding the flatten layer to flat the features\n",
    "model_2.add(Flatten())\n",
    "\n",
    "# Add a dense layer ( or fully conected)\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "# Dropout to reduce the number of neurons\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "# Adding the final layer to predict the classes\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b772b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vicente\\Documents\\COURSERA\\venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 137s 87ms/step - loss: 1.5833 - accuracy: 0.4258 - val_loss: 1.3902 - val_accuracy: 0.5089\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 135s 86ms/step - loss: 1.1981 - accuracy: 0.5765 - val_loss: 1.0603 - val_accuracy: 0.6276\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.0271 - accuracy: 0.6400 - val_loss: 0.9707 - val_accuracy: 0.6618\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 141s 90ms/step - loss: 0.9253 - accuracy: 0.6768 - val_loss: 0.8958 - val_accuracy: 0.6954\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 139s 89ms/step - loss: 0.8627 - accuracy: 0.7028 - val_loss: 0.8162 - val_accuracy: 0.7222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18420fc3490>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt_2 = keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac328a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
